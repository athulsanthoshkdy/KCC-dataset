{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dab4edb9",
        "outputId": "46d5f824-1fc6-4b94-be50-d288f6c45dfd"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "dab4edb9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e794efe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e794efe",
        "outputId": "067febb7-7bbe-4be7-d6b4-f7bbd01cd745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ParquetReader\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ca25cf0",
        "outputId": "76d3a15c-4a14-4811-d84a-226a9dbfeec9"
      },
      "source": [
        "# Define the path to the folder in your Google Drive\n",
        "folder_path = '/content/drive/MyDrive/kcc_all_states_combined.parquet'\n",
        "\n",
        "# Read the parquet files from the folder using Spark\n",
        "try:\n",
        "    spark_df = spark.read.parquet(folder_path)\n",
        "\n",
        "    print(f\"Successfully loaded data from {folder_path} into a Spark DataFrame.\")\n",
        "    print(\"\\nInitial Data Inspection (Spark DataFrame):\")\n",
        "    spark_df.printSchema()\n",
        "    spark_df.show(5)\n",
        "    print(f\"\\nNumber of rows: {spark_df.count()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the parquet files with Spark: {e}\")"
      ],
      "id": "0ca25cf0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from /content/drive/MyDrive/kcc_all_states_combined.parquet into a Spark DataFrame.\n",
            "\n",
            "Initial Data Inspection (Spark DataFrame):\n",
            "root\n",
            " |-- StateName: string (nullable = true)\n",
            " |-- DistrictName: string (nullable = true)\n",
            " |-- BlockName: string (nullable = true)\n",
            " |-- Season: string (nullable = true)\n",
            " |-- Sector: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Crop: string (nullable = true)\n",
            " |-- QueryType: string (nullable = true)\n",
            " |-- QueryText: string (nullable = true)\n",
            " |-- KccAns: string (nullable = true)\n",
            " |-- CreatedOn: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- source_file: string (nullable = true)\n",
            "\n",
            "An error occurred while reading the parquet files with Spark: An error occurred while calling o28.showString.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (320c13dda424 executor driver): java.io.IOException: Transport endpoint is not connected\n",
            "\tat java.base/java.io.FileDescriptor.close0(Native Method)\n",
            "\tat java.base/java.io.FileDescriptor.close(FileDescriptor.java:297)\n",
            "\tat java.base/java.io.FileInputStream$1.close(FileInputStream.java:375)\n",
            "\tat java.base/java.io.FileDescriptor.closeAll(FileDescriptor.java:355)\n",
            "\tat java.base/java.io.FileInputStream.close(FileInputStream.java:373)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.close(RawLocalFileSystem.java:177)\n",
            "\tat java.base/java.io.BufferedInputStream.close(BufferedInputStream.java:489)\n",
            "\tat java.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n",
            "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.close(ChecksumFileSystem.java:220)\n",
            "\tat java.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n",
            "\tat org.apache.parquet.io.DelegatingSeekableInputStream.close(DelegatingSeekableInputStream.java:50)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:213)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n",
            "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n",
            "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
            "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n",
            "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n",
            "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n",
            "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.io.IOException: Transport endpoint is not connected\n",
            "\tat java.base/java.io.FileDescriptor.close0(Native Method)\n",
            "\tat java.base/java.io.FileDescriptor.close(FileDescriptor.java:297)\n",
            "\tat java.base/java.io.FileInputStream$1.close(FileInputStream.java:375)\n",
            "\tat java.base/java.io.FileDescriptor.closeAll(FileDescriptor.java:355)\n",
            "\tat java.base/java.io.FileInputStream.close(FileInputStream.java:373)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.close(RawLocalFileSystem.java:177)\n",
            "\tat java.base/java.io.BufferedInputStream.close(BufferedInputStream.java:489)\n",
            "\tat java.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n",
            "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.close(ChecksumFileSystem.java:220)\n",
            "\tat java.base/java.io.FilterInputStream.close(FilterInputStream.java:180)\n",
            "\tat org.apache.parquet.io.DelegatingSeekableInputStream.close(DelegatingSeekableInputStream.java:50)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n",
            "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
            "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:213)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\t... 1 more\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f852ebc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f852ebc",
        "outputId": "ff61b6ea-95ab-4192-ade7-595929409268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "/tmp/ipython-input-2621019220.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  folder_path = 'D:\\Scripts\\Kissan Dataset\\kcc_all_states_combined.parquet'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while reading the parquet files with Spark: java.net.URISyntaxException: Relative path in absolute URI: D:%5CScripts%5CKissan%20Dataset%5Ckcc_all_states_combined.parquet\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the folder in your Google Drive\n",
        "folder_path = 'D:\\Scripts\\Kissan Dataset\\kcc_all_states_combined.parquet'\n",
        "\n",
        "# Read the parquet files from the folder using Spark\n",
        "try:\n",
        "    spark_df = spark.read.parquet(folder_path)\n",
        "\n",
        "    print(f\"Successfully loaded data from {folder_path} into a Spark DataFrame.\")\n",
        "    print(\"\\nInitial Data Inspection (Spark DataFrame):\")\n",
        "    spark_df.printSchema()\n",
        "    spark_df.show(5)\n",
        "    print(f\"\\nNumber of rows: {spark_df.count()}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the parquet files with Spark: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb90bc64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb90bc64",
        "outputId": "9f7ef9d9-d32b-40ed-cd86-948916eac5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+--------+--------+--------+--------+---------+---------+--------+---------+--------+--------+-----------+\n",
            "|StateName|DistrictName|BlockName|  Season|  Sector|Category|    Crop|QueryType|QueryText|  KccAns|CreatedOn|    year|   month|source_file|\n",
            "+---------+------------+---------+--------+--------+--------+--------+---------+---------+--------+---------+--------+--------+-----------+\n",
            "|    14362|    24863620| 26347809|26880217|29885686|30747830|31102588| 31278797| 31383472|38250028| 45351733|45385186|45412518|          0|\n",
            "+---------+------------+---------+--------+--------+--------+--------+---------+---------+--------+---------+--------+--------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec873ab0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec873ab0",
        "outputId": "8dfa898e-c444-45da-9874-1fb00ac7b49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------+------+------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----+--------------------+\n",
            "|           StateName|        DistrictName|     BlockName|Season|      Sector|  Category|                Crop|           QueryType|           QueryText|              KccAns|           CreatedOn|year|month|         source_file|\n",
            "+--------------------+--------------------+--------------+------+------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----+--------------------+\n",
            "|              ODISHA|            NAYAGARH|     DASAPALLA|    NA| AGRICULTURE|    Others|              Others|  Government Schemes|Mandi registratio...|Advised to contac...|2023-02-03T14:09:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|             CUTTACK|TANGI CHOUDWAR|    NA|HORTICULTURE|   Flowers|   Hibiscus (Gurhal)|\\tPlant Protection\\t|White mealy bug i...|Recommended to sp...|2023-02-03T14:16:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|             BHADRAK|       BHADRAK|    NA|HORTICULTURE|    Fruits|               Mango| Nutrient Management|About planofix ( ...|Planofix ( Alpha ...|2023-02-03T15:10:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|           KALAHANDI|        NARALA|    NA| AGRICULTURE|   Cereals|        Paddy (Dhan)|\\tPlant Protection\\t|Gall midge infest...|Recommended to sp...|                NULL|NULL| NULL|file:///D:/Script...|\n",
            "|Dose of Zinc sulp...|2023-02-04T11:12:...|          2023|     2|        NULL|      NULL|                NULL|                NULL|                NULL|                NULL|                NULL|NULL| NULL|file:///D:/Script...|\n",
            "|              ODISHA|             BARGARH|       SOHELLA|    NA| AGRICULTURE|   Cereals|        Paddy (Dhan)|\\tPlant Protection\\t|Leaf folder in paddy|Recommended to sp...|2023-02-04T11:50:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|                PURI|         KANAS|    NA|HORTICULTURE|Vegetables|            Chillies|\\tPlant Protection\\t|leaf curling in c...|Recommended to sp...|2023-02-03T18:57:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|                PURI|         KANAS|    NA|HORTICULTURE|Vegetables|             Pumpkin| Nutrient Management|Better flower & f...|Recommended to sp...|2023-02-03T19:02:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|            NAWAPARA|      SINAPALI|    NA| AGRICULTURE|  Oilseeds|Sunflower (suryam...|  Cultural Practices|Sunflower duratio...|Advised that sunf...|2023-02-03T19:10:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|            BALASORE|      BALIAPAL|    NA| AGRICULTURE|   Cereals|        Paddy (Dhan)|\\tPlant Protection\\t|Way of applicatio...|Recommended to sp...|2023-02-04T07:44:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|           KALAHANDI|      JUNAGARH|    NA| AGRICULTURE|    Others|              Others|  Market Information|Mandi token relat...|Advised to contac...|2023-02-04T07:53:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|             BHADRAK|    DHAMANAGAR|    NA| AGRICULTURE|    Others|              Others|  Market Information| Mandi related issue|Advised  to consu...|2023-02-04T08:17:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|             BARGARH|      GAISILET|    NA| AGRICULTURE|   Cereals|        Paddy (Dhan)|     Weed Management|Algae in paddy fi...|--Recommended to ...|2023-02-02T17:56:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|             CUTTACK|TANGI CHOUDWAR|    NA|HORTICULTURE|Vegetables|            Chillies|\\tPlant Protection\\t|Green peach aphid...|Recommended to sp...|2023-02-02T15:38:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|                PURI|        PIPILI|    NA| AGRICULTURE|    Pulses|Black Gram (urd b...|Fertilizer Use an...|Fertilizer Dose o...|Fertilizer Dose o...|2023-02-02T15:41:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|      JAGATSINGHAPUR|        TIRTOL|    NA|HORTICULTURE|Vegetables|        Bitter Gourd|\\tPlant Protection\\t|Chewing pest in b...|Recommended to sp...|2023-02-02T15:47:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|          MAYURBHANJ|       KULIANA|    NA| AGRICULTURE|   Millets|       Maize (Makka)|\\tPlant Protection\\t|Leaf Folder in Ma...|Recommended to sp...|                NULL|NULL| NULL|file:///D:/Script...|\n",
            "|Recommended to sp...|2023-02-02T15:55:...|          2023|     2|        NULL|      NULL|                NULL|                NULL|                NULL|                NULL|                NULL|NULL| NULL|file:///D:/Script...|\n",
            "|              ODISHA|            BALASORE|       BHOGRAI|    NA|HORTICULTURE|Vegetables|Bhindi(Okra/Ladys...|\\tPlant Protection\\t| Shoot borer in okra|Recommended to ap...|2023-02-02T16:15:...|2023|    2|file:///D:/Script...|\n",
            "|              ODISHA|            KEONJHAR|        PATANA|    NA|HORTICULTURE|Vegetables|             Pumpkin|\\tPlant Protection\\t|chewing pest at 2...| Recommended to a...|2023-02-02T16:18:...|2023|    2|file:///D:/Script...|\n",
            "+--------------------+--------------------+--------------+------+------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from functools import reduce\n",
        "\n",
        "# Create a list of columns to check for emptiness, excluding 'source_files'\n",
        "columns_to_check = [c for c in spark_df.columns if c != 'source_file']\n",
        "\n",
        "# Build the condition for rows where all the columns in 'columns_to_check' are null or empty\n",
        "condition = (col(c).isNull() | (col(c) == '') for c in columns_to_check)\n",
        "\n",
        "# Combine all the conditions using the `&` (AND) operator\n",
        "combined_condition = reduce(lambda x, y: x & y, condition)\n",
        "\n",
        "# Filter the rows where at least one column in 'columns_to_check' is not null or empty\n",
        "filtered_df = spark_df.filter(~combined_condition)\n",
        "\n",
        "# Show the result\n",
        "filtered_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b86c3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17b86c3a",
        "outputId": "adb938d2-a9d8-48ad-f974-757fbca0d795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+--------+--------+--------+--------+---------+---------+--------+---------+--------+--------+-----------+\n",
            "|StateName|DistrictName|BlockName|  Season|  Sector|Category|    Crop|QueryType|QueryText|  KccAns|CreatedOn|    year|   month|source_file|\n",
            "+---------+------------+---------+--------+--------+--------+--------+---------+---------+--------+---------+--------+--------+-----------+\n",
            "|    12342|    24861600| 26345789|26878197|29883666|30745810|31100568| 31276777| 31381452|38248008| 45349713|45383166|45410498|          0|\n",
            "+---------+------------+---------+--------+--------+--------+--------+---------+---------+--------+---------+--------+--------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "filtered_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in filtered_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfa0046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cfa0046",
        "outputId": "7303e02b-b618-4168-876c-6e7dd123fcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of rows: 79257331\n",
            "Number of rows after dropping missing QueryText/KccAns: 41004159\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Drop rows where QueryText is null, empty string, or \"NULL\" OR KccAns is null, empty string, or \"NULL\"\n",
        "cleaned_spark_df = filtered_df.filter(\n",
        "    ~((col(\"QueryText\").isNull()) | (col(\"QueryText\") == \"\") | (col(\"QueryText\") == \"NULL\")) &\n",
        "    ~((col(\"KccAns\").isNull()) | (col(\"KccAns\") == \"\") | (col(\"KccAns\") == \"NULL\"))\n",
        ")\n",
        "\n",
        "print(f\"Original number of rows: {filtered_df.count()}\")\n",
        "print(f\"Number of rows after dropping missing QueryText/KccAns: {cleaned_spark_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3901759",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3901759",
        "outputId": "e7907bca-197a-4d6c-9ddf-c5c7093b8f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|StateName|DistrictName|BlockName|Season|Sector|Category|Crop|QueryType|QueryText|KccAns|CreatedOn|   year|  month|source_file|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|      164|         355|      123|   633|   359|    2458| 818|     2505|        0|     0|  7267178|7300552|7327698|          0|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "cleaned_spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in cleaned_spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c294e9e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c294e9e2",
        "outputId": "672ed99c-9d7d-47f1-920f-1b01a1dd0ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of rows: 41004159\n",
            "Number of rows after dropping 'test' or 'wrong number' in KccAns: 40966773\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "\n",
        "# Define keywords to filter out (converted to lowercase)\n",
        "print(f\"Original number of rows: {cleaned_spark_df.count()}\")\n",
        "keywords_to_drop = [\"wrong number\", \"test call\"]\n",
        "\n",
        "# Create a condition to filter OUT rows where KccAns_cleaned contains any of the keywords\n",
        "# We'll convert KccAns_cleaned to lowercase for case-insensitive matching\n",
        "filter_condition_to_drop = lower(col(\"KccAns\")).contains(keywords_to_drop[0])\n",
        "for keyword in keywords_to_drop[1:]:\n",
        "    filter_condition_to_drop = filter_condition_to_drop | lower(col(\"KccAns\")).contains(keyword)\n",
        "\n",
        "# Filter the DataFrame to keep rows that DO NOT contain the keywords\n",
        "cleaned_spark_df = cleaned_spark_df.filter(~filter_condition_to_drop)\n",
        "\n",
        "# Print the row counts before and after the filtering\n",
        "\n",
        "print(f\"Number of rows after dropping 'test' or 'wrong number' in KccAns: {cleaned_spark_df.count()}\")\n",
        "\n",
        "# Optionally, you can save the updated DataFrame back to a file, if needed\n",
        "# Example: Save it in Parquet format\n",
        "# cleaned_spark_df.write.parquet(\"path_to_save/cleaned_spark_df.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45b8d7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d45b8d7c",
        "outputId": "f895a9dc-a2aa-46cc-f49a-d4a6791265cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|StateName|DistrictName|BlockName|Season|Sector|Category|Crop|QueryType|QueryText|KccAns|CreatedOn|   year|  month|source_file|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|      164|         355|      123|   633|   359|    2458| 818|     2505|        0|     0|  7267113|7300487|7327633|          0|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "cleaned_spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in cleaned_spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2557e71a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2557e71a",
        "outputId": "1c8fca88-848e-41ca-95b0-4152fb8eb378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample values from 'source_file' column:\n",
            "+------------------------------------------------------------------+\n",
            "|source_file                                                       |\n",
            "+------------------------------------------------------------------+\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "+------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Sample values after attempting to populate StateName:\n",
            "+---------+------------------------------------------------------------------+\n",
            "|StateName|source_file                                                       |\n",
            "+---------+------------------------------------------------------------------+\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "+---------+------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Number of rows after attempting to populate StateName: 40966773\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, split, element_at, when\n",
        "\n",
        "# Show a sample of the 'source_file' column to understand its structure\n",
        "print(\"Sample values from 'source_file' column:\")\n",
        "cleaned_spark_df.select(\"source_file\").show(20, truncate=False)\n",
        "\n",
        "# Let's assume the state name is part of the path, just before the filename\n",
        "# Extract the state name from the 'source_file' column\n",
        "cleaned_spark_df = cleaned_spark_df.withColumn(\n",
        "    \"extracted_state\",\n",
        "    when(col(\"source_file\").isNotNull(), element_at(split(col(\"source_file\"), \"/\"), -2)).otherwise(None)\n",
        ")\n",
        "\n",
        "# Update 'StateName' only where it's null, empty, or 'NULL', using the extracted state name\n",
        "cleaned_spark_df = cleaned_spark_df.withColumn(\n",
        "    \"StateName\",\n",
        "    when(\n",
        "        (col(\"StateName\").isNull()) | (col(\"StateName\") == \"\") | (col(\"StateName\") == \"NULL\"),\n",
        "        col(\"extracted_state\")\n",
        "    ).otherwise(col(\"StateName\"))\n",
        ").drop(\"extracted_state\")  # Drop the temporary 'extracted_state' column\n",
        "\n",
        "# Show a sample of the updated 'StateName' and 'source_file'\n",
        "print(\"\\nSample values after attempting to populate StateName:\")\n",
        "cleaned_spark_df.select(\"StateName\", \"source_file\").show(20, truncate=False)\n",
        "\n",
        "# Print the number of rows after updating the 'StateName'\n",
        "print(f\"\\nNumber of rows after attempting to populate StateName: {cleaned_spark_df.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19729d21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19729d21",
        "outputId": "19f4e734-7211-4617-f2b6-d31334446baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|StateName|DistrictName|BlockName|Season|Sector|Category|Crop|QueryType|QueryText|KccAns|CreatedOn|   year|  month|source_file|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|        0|         355|      123|   633|   359|    2458| 818|     2505|        0|     0|  7267113|7300487|7327633|          0|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "cleaned_spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in cleaned_spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972f6ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972f6ef2",
        "outputId": "49325bcd-db42-4482-9e71-edc8e691490c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows after filtering valid StateNames: 40757584\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+------------------------------------------------------------------+\n",
            "|StateName|DistrictName  |BlockName     |Season|Sector      |Category  |Crop                    |QueryType                      |QueryText                                                   |KccAns                                                                                                                                                                         |CreatedOn              |year|month|source_file                                                       |\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+------------------------------------------------------------------+\n",
            "|ODISHA   |NAYAGARH      |DASAPALLA     |NA    |AGRICULTURE |Others    |Others                  |Government Schemes             |Mandi registration related                                  |Advised to contact mandi helpline no.1967                                                                                                                                      |2023-02-03T14:09:14.853|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |CUTTACK       |TANGI CHOUDWAR|NA    |HORTICULTURE|Flowers   |Hibiscus (Gurhal)       |\\tPlant Protection\\t           |White mealy bug in Hibiscus flower                          |Recommended to spray Acephate 75 SP @ 200 gm per 200 litre of water per 1 acre of land.                                                                                        |2023-02-03T14:16:45.627|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BHADRAK       |BHADRAK       |NA    |HORTICULTURE|Fruits    |Mango                   |Nutrient Management            |About planofix ( Alpha naphthyl acetic acid 4.5% SL ) .     |Planofix ( Alpha naphthyl acetic acid 4.5% SL )  @ 3ml per 15 litre of water  and 6 ml per 15 litre for Mango.                                                                 |2023-02-03T15:10:34.647|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |KALAHANDI     |NARALA        |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Gall midge infestation in paddy                             |Recommended to spray Fipronil 5% SC @ 400 ml per 200 litre of water per 1 acre of land.                                                                                        |NULL                   |NULL|NULL |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BARGARH       |SOHELLA       |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Leaf folder in paddy                                        |Recommended to spray Indoxacarb 15.8% EC @ 120 ml in 200 Litre of water per acre of land (6 ml in 15 Litre water) to control Leaf folder in Paddy.                             |2023-02-04T11:50:13.34 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |PURI          |KANAS         |NA    |HORTICULTURE|Vegetables|Chillies                |\\tPlant Protection\\t           |leaf curling in chillies                                    |Recommended to spray Thiomethoxam 25% WG @ 80 gm per 200 litre of water per 1 acre of land or 6 gm in 15 liter of water.                                                       |2023-02-03T18:57:32.763|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |PURI          |KANAS         |NA    |HORTICULTURE|Vegetables|Pumpkin                 |Nutrient Management            |Better flower & fruit setting in pumpkin                    |Recommended to spray Biovita liquid @ 1ml per liter of water/15 ml in 15 liter of water.                                                                                       |2023-02-03T19:02:35.48 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |NAWAPARA      |SINAPALI      |NA    |AGRICULTURE |Oilseeds  |Sunflower (suryamukhi)  |Cultural Practices             |Sunflower duration related query                            |Advised that sunflower duration is 90-100 days                                                                                                                                 |2023-02-03T19:10:06.393|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BALASORE      |BALIAPAL      |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Way of application of saathi pre-emergence herbicide in rice|Recommended to spray Pyrazosulphuron ethyl 10% wp ( saathi ) @ 80 gm per 200 litre of water per 1 acre of land or apply with 20 kg of sand, after 2-3 days after transplanting.|2023-02-04T07:44:18.58 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |KALAHANDI     |JUNAGARH      |NA    |AGRICULTURE |Others    |Others                  |Market Information             |Mandi token related issue                                   |Advised to contact with mandi helpline number 1967 for mandi token related issue.                                                                                              |2023-02-04T07:53:55.59 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BHADRAK       |DHAMANAGAR    |NA    |AGRICULTURE |Others    |Others                  |Market Information             |Mandi related issue                                         |Advised  to consult the district agriculture officer.                                                                                                                          |2023-02-04T08:17:31.967|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BARGARH       |GAISILET      |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |Weed Management                |Algae in paddy fields                                       |--Recommended to apply Copper sulphate @ 4 kg in 20 kg of sand per acre to control Blue green Algae in Paddy field.                                                            |2023-02-02T17:56:30.833|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |CUTTACK       |TANGI CHOUDWAR|NA    |HORTICULTURE|Vegetables|Chillies                |\\tPlant Protection\\t           |Green peach aphid in chillies                               |Recommended to spray Fipronil 5% SC @ 400 ml per 200 litre of water per 1 acre of land / neem oil 3-5 ml per liter of water + soap powder then mix well then spray.            |2023-02-02T15:38:02.43 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |PURI          |PIPILI        |NA    |AGRICULTURE |Pulses    |Black Gram (urd bean)   |Fertilizer Use and Availability|Fertilizer Dose of Black Gram                               |Fertilizer Dose of black gram : DAP 34 kg + Urea 3 kg + MOP 13 kg                                                                                                              |2023-02-02T15:41:24.15 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |JAGATSINGHAPUR|TIRTOL        |NA    |HORTICULTURE|Vegetables|Bitter Gourd            |\\tPlant Protection\\t           |Chewing pest in bitter gourd                                |Recommended to spray Emamectin Benzoate 5 % SG@6gm per 15 litres of water                                                                                                      |2023-02-02T15:47:41.59 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |MAYURBHANJ    |KULIANA       |NA    |AGRICULTURE |Millets   |Maize (Makka)           |\\tPlant Protection\\t           |Leaf Folder in Maize.                                       |Recommended to spray Indoxacarb 15.8% EC @ 120 ml in 200 Litre of water per acre of land (6 ml in 15 Litre water) to control Leaf folder in Maize.                             |NULL                   |NULL|NULL |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BALASORE      |BHOGRAI       |NA    |HORTICULTURE|Vegetables|Bhindi(Okra/Ladysfinger)|\\tPlant Protection\\t           |Shoot borer in okra                                         |Recommended to apply spinosad 45 sc @ 6 ml per 15 litre of water to control the shoot borer in okra.                                                                           |2023-02-02T16:15:42.993|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |KEONJHAR      |PATANA        |NA    |HORTICULTURE|Vegetables|Pumpkin                 |\\tPlant Protection\\t           |chewing pest at 2 leaf stage of pumpkin                     | Recommended to apply Carbofuran 3G @ 5 gm per plant as ring methord / can spray neem oil 3-5 ml per liter of water + soap powder.                                             |2023-02-02T16:18:11.58 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BALANGIR      |GUDVELLA      |NA    |HORTICULTURE|Vegetables|Cucumber                |Seeds and Planting Material    |Varieties of Cucumber                                       |Varieties of cucumber - VNR ( Krish, Kumud) , Nuziveedu seeds ( Karisma, Kiran, Shalini), Tata ( Zara, Naira,003) , Japanese long , Poinsett                                   |NULL                   |NULL|NULL |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |NAYAGARH      |DASAPALLA     |NA    |AGRICULTURE |Others    |Others                  |Government Schemes             |New apply Kalia Scheme                                      |Advised to contact CSC center for new apply of kalia scheme but the site is close now so wait some days.                                                                       |2023-02-02T20:18:38.963|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# List of valid state names\n",
        "valid_state_names = [\n",
        "    \"UTTAR PRADESH\", \"RAJASTHAN\", \"MAHARASHTRA\", \"MADHYA PRADESH\",\n",
        "    \"HARYANA\", \"PUNJAB\", \"GUJARAT\", \"BIHAR\", \"TAMILNADU\", \"KARNATAKA\",\n",
        "    \"ODISHA\", \"WEST BENGAL\", \"ANDHRA PRADESH\", \"TELANGANA\",\n",
        "    \"HIMACHAL PRADESH\", \"CHHATTISGARH\", \"JAMMU AND KASHMIR\",\n",
        "    \"JHARKAND\", \"UTTARAKHAND\", \"ASSAM\", \"KERALA\", \"DELHI\", \"TRIPURA\",\n",
        "    \"PUDUCHERRY\", \"MANIPUR\", \"MIZORAM\", \"MEGHALAYA\", \"GOA\", \"SIKKIM\",\n",
        "    \"ARUNACHAL PRADESH\", \"NAGALAND\", \"A AND N ISLANDS\", \"CHANDIGARH\",\n",
        "    \"LAKSHADWEEP\", \"0\", \"DADRA AND NAGAR HAVELI\", \"DAMAN AND DIU\"\n",
        "]\n",
        "\n",
        "# Filter the DataFrame to keep only rows where StateName is in the valid list\n",
        "cleaned_spark_df = cleaned_spark_df.filter(col(\"StateName\").isin(valid_state_names))\n",
        "\n",
        "# Show the result\n",
        "print(f\"Number of rows after filtering valid StateNames: {cleaned_spark_df.count()}\")\n",
        "cleaned_spark_df.show(20, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97add1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e97add1b",
        "outputId": "3f618583-7121-4836-9e1f-ceb7eb9e2ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|StateName|DistrictName|BlockName|Season|Sector|Category|Crop|QueryType|QueryText|KccAns|CreatedOn|   year|  month|source_file|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|        0|           0|        0|     0|     0|       0|   0|        3|        0|     0|  7209172|7210199|7210520|          0|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "cleaned_spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in cleaned_spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b240799",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b240799",
        "outputId": "b8e88678-cfbe-497c-b52a-d32b06c8c900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------------+------------+------+-----------+--------+------+---------+-----------------+-----------------+-----------------------+----+-----+-------------------------------------------------------------------------+\n",
            "|StateName    |DistrictName              |BlockName   |Season|Sector     |Category|Crop  |QueryType|QueryText        |KccAns           |CreatedOn              |year|month|source_file                                                              |\n",
            "+-------------+--------------------------+------------+------+-----------+--------+------+---------+-----------------+-----------------+-----------------------+----+-----+-------------------------------------------------------------------------+\n",
            "|UTTAR PRADESH|FATEHPUR                  |DHATA       |NA    |AGRICULTURE|Cereals |Wheat |NULL     |OTHER STATE CALL |OTHER STATE CALL |2025-07-16T10:21:42.5  |2025|7    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_UTTAR_PRADESH.csv|\n",
            "|UTTAR PRADESH|AMETHI ( Shahu Ji Maharaj)|null        |NA    |AGRICULTURE|Others  |Others|NULL     |t                |--t              |2025-07-19T17:51:43.87 |2025|7    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_UTTAR_PRADESH.csv|\n",
            "|BIHAR        |DARBHANGA                 |GHANSHYAMPUR|NA    |AGRICULTURE|0       |0     |NULL     |OTHER STATE  CALL|OTHER STATE  CALL|2025-07-21T09:09:54.483|2025|7    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_BIHAR.csv        |\n",
            "+-------------+--------------------------+------------+------+-----------+--------+------+---------+-----------------+-----------------+-----------------------+----+-----+-------------------------------------------------------------------------+\n",
            "\n",
            "Current total row count: 40757584\n",
            "Number of rows with null 'QueryType': 3\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Get the current total row count\n",
        "total_row_count = cleaned_spark_df.count()\n",
        "\n",
        "# Filter rows where 'QueryType' is null\n",
        "df_query_type_null = cleaned_spark_df.filter(col(\"QueryType\").isNull())\n",
        "\n",
        "# Show the rows where 'QueryType' is null\n",
        "df_query_type_null.show(20, truncate=False)\n",
        "\n",
        "# Print the total row count and the number of rows with null 'QueryType'\n",
        "print(f\"Current total row count: {total_row_count}\")\n",
        "print(f\"Number of rows with null 'QueryType': {df_query_type_null.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14ab328",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c14ab328",
        "outputId": "f8345576-adc0-4705-cdb5-1cabfaeff97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+------------------------------------------------------------------+\n",
            "|StateName|DistrictName  |BlockName     |Season|Sector      |Category  |Crop                    |QueryType                      |QueryText                                                   |KccAns                                                                                                                                                                         |CreatedOn              |year|month|source_file                                                       |\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+------------------------------------------------------------------+\n",
            "|ODISHA   |NAYAGARH      |DASAPALLA     |NA    |AGRICULTURE |Others    |Others                  |Government Schemes             |Mandi registration related                                  |Advised to contact mandi helpline no.1967                                                                                                                                      |2023-02-03T14:09:14.853|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |CUTTACK       |TANGI CHOUDWAR|NA    |HORTICULTURE|Flowers   |Hibiscus (Gurhal)       |\\tPlant Protection\\t           |White mealy bug in Hibiscus flower                          |Recommended to spray Acephate 75 SP @ 200 gm per 200 litre of water per 1 acre of land.                                                                                        |2023-02-03T14:16:45.627|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BHADRAK       |BHADRAK       |NA    |HORTICULTURE|Fruits    |Mango                   |Nutrient Management            |About planofix ( Alpha naphthyl acetic acid 4.5% SL ) .     |Planofix ( Alpha naphthyl acetic acid 4.5% SL )  @ 3ml per 15 litre of water  and 6 ml per 15 litre for Mango.                                                                 |2023-02-03T15:10:34.647|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |KALAHANDI     |NARALA        |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Gall midge infestation in paddy                             |Recommended to spray Fipronil 5% SC @ 400 ml per 200 litre of water per 1 acre of land.                                                                                        |NULL                   |NULL|NULL |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BARGARH       |SOHELLA       |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Leaf folder in paddy                                        |Recommended to spray Indoxacarb 15.8% EC @ 120 ml in 200 Litre of water per acre of land (6 ml in 15 Litre water) to control Leaf folder in Paddy.                             |2023-02-04T11:50:13.34 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |PURI          |KANAS         |NA    |HORTICULTURE|Vegetables|Chillies                |\\tPlant Protection\\t           |leaf curling in chillies                                    |Recommended to spray Thiomethoxam 25% WG @ 80 gm per 200 litre of water per 1 acre of land or 6 gm in 15 liter of water.                                                       |2023-02-03T18:57:32.763|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |PURI          |KANAS         |NA    |HORTICULTURE|Vegetables|Pumpkin                 |Nutrient Management            |Better flower & fruit setting in pumpkin                    |Recommended to spray Biovita liquid @ 1ml per liter of water/15 ml in 15 liter of water.                                                                                       |2023-02-03T19:02:35.48 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |NAWAPARA      |SINAPALI      |NA    |AGRICULTURE |Oilseeds  |Sunflower (suryamukhi)  |Cultural Practices             |Sunflower duration related query                            |Advised that sunflower duration is 90-100 days                                                                                                                                 |2023-02-03T19:10:06.393|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BALASORE      |BALIAPAL      |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Way of application of saathi pre-emergence herbicide in rice|Recommended to spray Pyrazosulphuron ethyl 10% wp ( saathi ) @ 80 gm per 200 litre of water per 1 acre of land or apply with 20 kg of sand, after 2-3 days after transplanting.|2023-02-04T07:44:18.58 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |KALAHANDI     |JUNAGARH      |NA    |AGRICULTURE |Others    |Others                  |Market Information             |Mandi token related issue                                   |Advised to contact with mandi helpline number 1967 for mandi token related issue.                                                                                              |2023-02-04T07:53:55.59 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BHADRAK       |DHAMANAGAR    |NA    |AGRICULTURE |Others    |Others                  |Market Information             |Mandi related issue                                         |Advised  to consult the district agriculture officer.                                                                                                                          |2023-02-04T08:17:31.967|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BARGARH       |GAISILET      |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |Weed Management                |Algae in paddy fields                                       |--Recommended to apply Copper sulphate @ 4 kg in 20 kg of sand per acre to control Blue green Algae in Paddy field.                                                            |2023-02-02T17:56:30.833|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |CUTTACK       |TANGI CHOUDWAR|NA    |HORTICULTURE|Vegetables|Chillies                |\\tPlant Protection\\t           |Green peach aphid in chillies                               |Recommended to spray Fipronil 5% SC @ 400 ml per 200 litre of water per 1 acre of land / neem oil 3-5 ml per liter of water + soap powder then mix well then spray.            |2023-02-02T15:38:02.43 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |PURI          |PIPILI        |NA    |AGRICULTURE |Pulses    |Black Gram (urd bean)   |Fertilizer Use and Availability|Fertilizer Dose of Black Gram                               |Fertilizer Dose of black gram : DAP 34 kg + Urea 3 kg + MOP 13 kg                                                                                                              |2023-02-02T15:41:24.15 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |JAGATSINGHAPUR|TIRTOL        |NA    |HORTICULTURE|Vegetables|Bitter Gourd            |\\tPlant Protection\\t           |Chewing pest in bitter gourd                                |Recommended to spray Emamectin Benzoate 5 % SG@6gm per 15 litres of water                                                                                                      |2023-02-02T15:47:41.59 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |MAYURBHANJ    |KULIANA       |NA    |AGRICULTURE |Millets   |Maize (Makka)           |\\tPlant Protection\\t           |Leaf Folder in Maize.                                       |Recommended to spray Indoxacarb 15.8% EC @ 120 ml in 200 Litre of water per acre of land (6 ml in 15 Litre water) to control Leaf folder in Maize.                             |NULL                   |NULL|NULL |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BALASORE      |BHOGRAI       |NA    |HORTICULTURE|Vegetables|Bhindi(Okra/Ladysfinger)|\\tPlant Protection\\t           |Shoot borer in okra                                         |Recommended to apply spinosad 45 sc @ 6 ml per 15 litre of water to control the shoot borer in okra.                                                                           |2023-02-02T16:15:42.993|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |KEONJHAR      |PATANA        |NA    |HORTICULTURE|Vegetables|Pumpkin                 |\\tPlant Protection\\t           |chewing pest at 2 leaf stage of pumpkin                     | Recommended to apply Carbofuran 3G @ 5 gm per plant as ring methord / can spray neem oil 3-5 ml per liter of water + soap powder.                                             |2023-02-02T16:18:11.58 |2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |BALANGIR      |GUDVELLA      |NA    |HORTICULTURE|Vegetables|Cucumber                |Seeds and Planting Material    |Varieties of Cucumber                                       |Varieties of cucumber - VNR ( Krish, Kumud) , Nuziveedu seeds ( Karisma, Kiran, Shalini), Tata ( Zara, Naira,003) , Japanese long , Poinsett                                   |NULL                   |NULL|NULL |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "|ODISHA   |NAYAGARH      |DASAPALLA     |NA    |AGRICULTURE |Others    |Others                  |Government Schemes             |New apply Kalia Scheme                                      |Advised to contact CSC center for new apply of kalia scheme but the site is close now so wait some days.                                                                       |2023-02-02T20:18:38.963|2023|2    |file:///D:/Scripts/Kissan%20Dataset/Dataset/kcc_dataset_ODISHA.csv|\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Number of rows after dropping 'QueryType' is null: 40757581\n"
          ]
        }
      ],
      "source": [
        "# Drop rows where 'QueryType' is null\n",
        "cleaned_spark_df = cleaned_spark_df.filter(col(\"QueryType\").isNotNull())\n",
        "\n",
        "# Show the updated DataFrame (first 20 rows)\n",
        "cleaned_spark_df.show(20, truncate=False)\n",
        "\n",
        "# Print the new row count after dropping rows with null 'QueryType'\n",
        "print(f\"Number of rows after dropping 'QueryType' is null: {cleaned_spark_df.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7154d746",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7154d746",
        "outputId": "674c79c9-20b1-4986-ecde-56d7d0e54548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|StateName|DistrictName|BlockName|Season|Sector|Category|Crop|QueryType|QueryText|KccAns|CreatedOn|   year|  month|source_file|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "|        0|           0|        0|     0|     0|       0|   0|        0|        0|     0|  7209172|7210199|7210520|          0|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "cleaned_spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in cleaned_spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cfa96e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76cfa96e",
        "outputId": "f9d968c0-2d49-4daa-b374-3dd9495df6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+\n",
            "|StateName|DistrictName  |BlockName     |Season|Sector      |Category  |Crop                    |QueryType                      |QueryText                                                   |KccAns                                                                                                                                                                         |CreatedOn              |year|month|\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+\n",
            "|ODISHA   |NAYAGARH      |DASAPALLA     |NA    |AGRICULTURE |Others    |Others                  |Government Schemes             |Mandi registration related                                  |Advised to contact mandi helpline no.1967                                                                                                                                      |2023-02-03T14:09:14.853|2023|2    |\n",
            "|ODISHA   |CUTTACK       |TANGI CHOUDWAR|NA    |HORTICULTURE|Flowers   |Hibiscus (Gurhal)       |\\tPlant Protection\\t           |White mealy bug in Hibiscus flower                          |Recommended to spray Acephate 75 SP @ 200 gm per 200 litre of water per 1 acre of land.                                                                                        |2023-02-03T14:16:45.627|2023|2    |\n",
            "|ODISHA   |BHADRAK       |BHADRAK       |NA    |HORTICULTURE|Fruits    |Mango                   |Nutrient Management            |About planofix ( Alpha naphthyl acetic acid 4.5% SL ) .     |Planofix ( Alpha naphthyl acetic acid 4.5% SL )  @ 3ml per 15 litre of water  and 6 ml per 15 litre for Mango.                                                                 |2023-02-03T15:10:34.647|2023|2    |\n",
            "|ODISHA   |KALAHANDI     |NARALA        |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Gall midge infestation in paddy                             |Recommended to spray Fipronil 5% SC @ 400 ml per 200 litre of water per 1 acre of land.                                                                                        |NULL                   |NULL|NULL |\n",
            "|ODISHA   |BARGARH       |SOHELLA       |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Leaf folder in paddy                                        |Recommended to spray Indoxacarb 15.8% EC @ 120 ml in 200 Litre of water per acre of land (6 ml in 15 Litre water) to control Leaf folder in Paddy.                             |2023-02-04T11:50:13.34 |2023|2    |\n",
            "|ODISHA   |PURI          |KANAS         |NA    |HORTICULTURE|Vegetables|Chillies                |\\tPlant Protection\\t           |leaf curling in chillies                                    |Recommended to spray Thiomethoxam 25% WG @ 80 gm per 200 litre of water per 1 acre of land or 6 gm in 15 liter of water.                                                       |2023-02-03T18:57:32.763|2023|2    |\n",
            "|ODISHA   |PURI          |KANAS         |NA    |HORTICULTURE|Vegetables|Pumpkin                 |Nutrient Management            |Better flower & fruit setting in pumpkin                    |Recommended to spray Biovita liquid @ 1ml per liter of water/15 ml in 15 liter of water.                                                                                       |2023-02-03T19:02:35.48 |2023|2    |\n",
            "|ODISHA   |NAWAPARA      |SINAPALI      |NA    |AGRICULTURE |Oilseeds  |Sunflower (suryamukhi)  |Cultural Practices             |Sunflower duration related query                            |Advised that sunflower duration is 90-100 days                                                                                                                                 |2023-02-03T19:10:06.393|2023|2    |\n",
            "|ODISHA   |BALASORE      |BALIAPAL      |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |\\tPlant Protection\\t           |Way of application of saathi pre-emergence herbicide in rice|Recommended to spray Pyrazosulphuron ethyl 10% wp ( saathi ) @ 80 gm per 200 litre of water per 1 acre of land or apply with 20 kg of sand, after 2-3 days after transplanting.|2023-02-04T07:44:18.58 |2023|2    |\n",
            "|ODISHA   |KALAHANDI     |JUNAGARH      |NA    |AGRICULTURE |Others    |Others                  |Market Information             |Mandi token related issue                                   |Advised to contact with mandi helpline number 1967 for mandi token related issue.                                                                                              |2023-02-04T07:53:55.59 |2023|2    |\n",
            "|ODISHA   |BHADRAK       |DHAMANAGAR    |NA    |AGRICULTURE |Others    |Others                  |Market Information             |Mandi related issue                                         |Advised  to consult the district agriculture officer.                                                                                                                          |2023-02-04T08:17:31.967|2023|2    |\n",
            "|ODISHA   |BARGARH       |GAISILET      |NA    |AGRICULTURE |Cereals   |Paddy (Dhan)            |Weed Management                |Algae in paddy fields                                       |--Recommended to apply Copper sulphate @ 4 kg in 20 kg of sand per acre to control Blue green Algae in Paddy field.                                                            |2023-02-02T17:56:30.833|2023|2    |\n",
            "|ODISHA   |CUTTACK       |TANGI CHOUDWAR|NA    |HORTICULTURE|Vegetables|Chillies                |\\tPlant Protection\\t           |Green peach aphid in chillies                               |Recommended to spray Fipronil 5% SC @ 400 ml per 200 litre of water per 1 acre of land / neem oil 3-5 ml per liter of water + soap powder then mix well then spray.            |2023-02-02T15:38:02.43 |2023|2    |\n",
            "|ODISHA   |PURI          |PIPILI        |NA    |AGRICULTURE |Pulses    |Black Gram (urd bean)   |Fertilizer Use and Availability|Fertilizer Dose of Black Gram                               |Fertilizer Dose of black gram : DAP 34 kg + Urea 3 kg + MOP 13 kg                                                                                                              |2023-02-02T15:41:24.15 |2023|2    |\n",
            "|ODISHA   |JAGATSINGHAPUR|TIRTOL        |NA    |HORTICULTURE|Vegetables|Bitter Gourd            |\\tPlant Protection\\t           |Chewing pest in bitter gourd                                |Recommended to spray Emamectin Benzoate 5 % SG@6gm per 15 litres of water                                                                                                      |2023-02-02T15:47:41.59 |2023|2    |\n",
            "|ODISHA   |MAYURBHANJ    |KULIANA       |NA    |AGRICULTURE |Millets   |Maize (Makka)           |\\tPlant Protection\\t           |Leaf Folder in Maize.                                       |Recommended to spray Indoxacarb 15.8% EC @ 120 ml in 200 Litre of water per acre of land (6 ml in 15 Litre water) to control Leaf folder in Maize.                             |NULL                   |NULL|NULL |\n",
            "|ODISHA   |BALASORE      |BHOGRAI       |NA    |HORTICULTURE|Vegetables|Bhindi(Okra/Ladysfinger)|\\tPlant Protection\\t           |Shoot borer in okra                                         |Recommended to apply spinosad 45 sc @ 6 ml per 15 litre of water to control the shoot borer in okra.                                                                           |2023-02-02T16:15:42.993|2023|2    |\n",
            "|ODISHA   |KEONJHAR      |PATANA        |NA    |HORTICULTURE|Vegetables|Pumpkin                 |\\tPlant Protection\\t           |chewing pest at 2 leaf stage of pumpkin                     | Recommended to apply Carbofuran 3G @ 5 gm per plant as ring methord / can spray neem oil 3-5 ml per liter of water + soap powder.                                             |2023-02-02T16:18:11.58 |2023|2    |\n",
            "|ODISHA   |BALANGIR      |GUDVELLA      |NA    |HORTICULTURE|Vegetables|Cucumber                |Seeds and Planting Material    |Varieties of Cucumber                                       |Varieties of cucumber - VNR ( Krish, Kumud) , Nuziveedu seeds ( Karisma, Kiran, Shalini), Tata ( Zara, Naira,003) , Japanese long , Poinsett                                   |NULL                   |NULL|NULL |\n",
            "|ODISHA   |NAYAGARH      |DASAPALLA     |NA    |AGRICULTURE |Others    |Others                  |Government Schemes             |New apply Kalia Scheme                                      |Advised to contact CSC center for new apply of kalia scheme but the site is close now so wait some days.                                                                       |2023-02-02T20:18:38.963|2023|2    |\n",
            "+---------+--------------+--------------+------+------------+----------+------------------------+-------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- StateName: string (nullable = true)\n",
            " |-- DistrictName: string (nullable = true)\n",
            " |-- BlockName: string (nullable = true)\n",
            " |-- Season: string (nullable = true)\n",
            " |-- Sector: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Crop: string (nullable = true)\n",
            " |-- QueryType: string (nullable = true)\n",
            " |-- QueryText: string (nullable = true)\n",
            " |-- KccAns: string (nullable = true)\n",
            " |-- CreatedOn: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Drop the 'source_file' column\n",
        "cleaned_spark_df = cleaned_spark_df.drop(\"source_file\")\n",
        "\n",
        "# Show the updated DataFrame (first 20 rows)\n",
        "cleaned_spark_df.show(20, truncate=False)\n",
        "\n",
        "# Print the schema to confirm that 'source_file' is dropped\n",
        "cleaned_spark_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66e9621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e66e9621",
        "outputId": "412effed-ac67-4dfe-a70c-6ad51143efab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing values (nulls) in all columns:\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+\n",
            "|StateName|DistrictName|BlockName|Season|Sector|Category|Crop|QueryType|QueryText|KccAns|CreatedOn|   year|  month|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+\n",
            "|        0|           0|        0|     0|     0|       0|   0|        0|        0|     0|  7209172|7210199|7210520|\n",
            "+---------+------------+---------+------+------+--------+----+---------+---------+------+---------+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Check for missing values (nulls) in each column\n",
        "print(\"Checking for missing values (nulls) in all columns:\")\n",
        "cleaned_spark_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in cleaned_spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0d6de95",
        "outputId": "65665543-a690-41a8-bc92-f208d654222f"
      },
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\"CreatedOn\", \"StateName\", \"DistrictName\", \"BlockName\", \"Sector\"]\n",
        "\n",
        "# Drop the specified columns from the DataFrame\n",
        "cleaned_spark_df = cleaned_spark_df.drop(*columns_to_drop)\n",
        "\n",
        "# Show the schema to confirm the columns have been dropped\n",
        "cleaned_spark_df.printSchema()\n",
        "\n",
        "# Show the first few rows of the updated DataFrame\n",
        "cleaned_spark_df.show(5)"
      ],
      "id": "a0d6de95",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Season: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Crop: string (nullable = true)\n",
            " |-- QueryType: string (nullable = true)\n",
            " |-- QueryText: string (nullable = true)\n",
            " |-- KccAns: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            "\n",
            "+------+--------+-----------------+--------------------+--------------------+--------------------+----+-----+\n",
            "|Season|Category|             Crop|           QueryType|           QueryText|              KccAns|year|month|\n",
            "+------+--------+-----------------+--------------------+--------------------+--------------------+----+-----+\n",
            "|    NA|  Others|           Others|  Government Schemes|Mandi registratio...|Advised to contac...|2023|    2|\n",
            "|    NA| Flowers|Hibiscus (Gurhal)|\\tPlant Protection\\t|White mealy bug i...|Recommended to sp...|2023|    2|\n",
            "|    NA|  Fruits|            Mango| Nutrient Management|About planofix ( ...|Planofix ( Alpha ...|2023|    2|\n",
            "|    NA| Cereals|     Paddy (Dhan)|\\tPlant Protection\\t|Gall midge infest...|Recommended to sp...|NULL| NULL|\n",
            "|    NA| Cereals|     Paddy (Dhan)|\\tPlant Protection\\t|Leaf folder in paddy|Recommended to sp...|2023|    2|\n",
            "+------+--------+-----------------+--------------------+--------------------+--------------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57e56d41",
        "outputId": "bc0e8e1c-329c-4d9e-f0d9-490946b2bb91"
      },
      "source": [
        "# Define the path in your Google Drive where you want to save the cleaned data\n",
        "output_drive_path = '/content/drive/My Drive/cleaned_kcc_dataset_sampled.parquet' # Replace with your desired path\n",
        "\n",
        "# Sample the DataFrame to reduce its size (e.g., keep 1/1000th of the data)\n",
        "sampled_spark_df = cleaned_spark_df.sample(withReplacement=False, fraction=0.001, seed=42)\n",
        "\n",
        "# Save the sampled_spark_df DataFrame to the specified path in Parquet format\n",
        "try:\n",
        "    sampled_spark_df.write.mode(\"overwrite\").parquet(output_drive_path)\n",
        "    print(f\"Successfully saved sampled_spark_df to {output_drive_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the DataFrame: {e}\")"
      ],
      "id": "57e56d41",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved sampled_spark_df to /content/drive/My Drive/cleaned_kcc_dataset_sampled.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3394028",
        "outputId": "5f884249-4aee-4856-e110-f62a6bc90d94"
      },
      "source": [
        "# Define the path in your Google Drive where you want to save the sampled data as CSV\n",
        "output_drive_path_csv = '/content/drive/My Drive/cleaned_kcc_dataset_sampled.csv' # Replace with your desired path for CSV\n",
        "\n",
        "# Save the sampled_spark_df DataFrame to the specified path in CSV format\n",
        "try:\n",
        "    sampled_spark_df.write.mode(\"overwrite\").csv(output_drive_path_csv, header=True)\n",
        "    print(f\"Successfully saved sampled_spark_df to {output_drive_path_csv}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the DataFrame as CSV: {e}\")"
      ],
      "id": "e3394028",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved sampled_spark_df to /content/drive/My Drive/cleaned_kcc_dataset_sampled.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}